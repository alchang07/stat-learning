---
title: "Lab 6"
author: "Alice Chang"
subtitle: Hands off my stack
output:
  pdf_document: default
  html_document:
    df_print: paged
---


* * *

#### Inventing a variable

```{r}
#load data
library(MASS)
library(dplyr)
library(ggplot2)

d <- read.csv("https://bit.ly/36kibHZ")
#add column MAPE to dataframe
MAPE <- d$Price / d$Earnings_10MA_back
d$MAPE <- MAPE
summary(MAPE)

#Remove all rows with missing data
d <- na.omit(d)

#build linear model with MAPE
m1 <- lm(Return_10_fwd ~ MAPE, data = d)
summary(m1)
#Calculate MSE for five-fold CV
set.seed(42)
d <- d[-sample(1:nrow(d), 4), ]
k <- 5
partition_index <- rep(1:k, each = nrow(d)/k) %>% 
sample()
MSE_i <- rep(NA, k)
d$partition_index <- partition_index
for(i in 1:k) {
  test <- d[d$partition_index == i, ]
  train <- d[d$partition_index != i,]
  lm1 <- lm(Return_10_fwd ~ MAPE, data = train)
  pd <- predict(lm1, newdata = test, type = "response") 
  MSE_i[i] <- mean((test$Return_10_fwd-pd)^2)

  }


MSE = mean(MSE_i)
MSE






```

1. There are exactly 120 NAs because "Earnings_10_MA_back" measures the ten-year moving average of earnings looking back from the current date. We do not have any earnings data for the previous ten years before 1871 that allows us to have information for the ten-year moving average for the first ten years. There is no data available for the ten-year moving average of earnings until 10 years later in 1871. Each row in the dataset represents one month of each of the 12 months in the year, and there are 10 years of mising data. Accordingly, 12 X 10 = 120, so there are exactly 120 NAs in both "Earnings_10MA_back" and "MAPE."

2. As evident in the linear model, the coefficient of MAPE is  -0.004602  and its standard error is 0.000173 . This means that for every unit increase in "MAPE" (the ratio of "Price,"  the index of U.S. stocks, to "Earnings_10MA_back," the ten year moving average of earnings), the average rate of return over the next ten years from the current data, "Return_10_fwd," should decrease by 0.004602  units. Moreovr, this rate of return can vary by 0.000173. Since P < 2e-16, the effect of MAPE is statistically significant. 

3. The MSE of the model under a five-fold CV is 0.001868484.  


#### Inverting a variable

```{r}
#Create inverted variable
Inv_MAPE <- 1 / d$MAPE
d$Inv_MAPE <- Inv_MAPE


#Fit model with Inv_MAPE
m2 <- lm(Return_10_fwd ~ Inv_MAPE, data = d)
summary(m2)

#Find five-fold CV MSE
MSE_i <- rep(NA, k)
for(i in 1:k) {
  test <- d[d$partition_index == i, ]
  train <- d[d$partition_index != i,]
  lm2 <- lm(Return_10_fwd ~ Inv_MAPE, data = train)
  pd2 <- predict(lm2, newdata = test, type = "response") 
  MSE_i[i] <- mean((test$Return_10_fwd-pd2)^2)

  }

MSE = mean(MSE_i)
MSE


```
1. As evident in the linear model, the coefficient of Inv_MAPE is 0.997627 and its standard error is 0.036567 . This means that for every unit increase in "Inv_MAPE" (the ratio of "Earnings+10MA_back," the ten year moving average of earnings to "Price,"  the index of U.S. stocks), the average rate of return over the next ten years from the current data, "Return_10_fwd," should increase by 0.997627 units. Moreovr, this rate of return can vary by 0.036567. Since P < 2e-16, the effect of Inv_MAPE is statistically significant. 

2. The CV MSE for this model is  0.001836749 which is slightly smaller than the CV MSE of the previous model. 

#### A simple model

```{r}
k <- 5
MSE_i <- rep(NA, k)

for(i in 1:k) {
  test <- d[d$partition_index == i, ]
  train <- d[d$partition_index != i,]
  MSE_i[i] <- mean((train$Return_10_fwd-train$Inv_MAPE)^2)
}

MSE = mean(MSE_i)
MSE

MSE_i <- rep(NA, k)

for(i in 1:k) {
  test <- d[d$partition_index == i, ]
  train <- d[d$partition_index != i,]
  MSE_i[i] <- mean((test$Return_10_fwd-test$Inv_MAPE)^2)

  }

MSE = mean(MSE_i)
MSE




```



1. The training MSE for this model is 0.001898626. This is exactly the same as the CV test MSE for this model. 

2. In most cases, we train our model on the training data and fit the derived model onto our test data afterwards, so that our training MSE may be lower than our test MSE at times. However, the training MSE and test MSE for this simple model are equivalent because we are not simply fitting a model derived from our training data onto the test data. Instead, in both the test and training data, we are evaluating the suggestion that 1/MAPE is equal to the average rate of return over the next ten years. Consequently, we would expect this MSE to be the same across the entire dataset. 

#### Is simple sufficient?

```{r}
betas <- rep(NA, 5000)
for(i in 1:5000) {
  boot_ind <- sample(1:nrow(d), 
                     size = nrow(d), 
                     replace = TRUE)
  d_boot <- d[boot_ind, ]
  betas[i] <- coef(lm(Return_10_fwd ~ Inv_MAPE, data = d_boot))[2]
}


#Graoh results
df <- data.frame(betas)

ggplot(df, aes(betas)) +
  geom_histogram(col = "white") +  theme_bw() + geom_vline(xintercept= 1, color="red")
 

#Obtain 95% bootstrap confidence interval
m_boot = lm(Return_10_fwd ~ Inv_MAPE, data = d_boot)
summary(m_boot)
confint(m_boot, level = 0.95)



confint(m2, level = 0.95)


```
The 95% bootstrap confidence interval for the slope of 1/MAPE is between  0.96444846 and  1.106315186. 
In comparision, through directly running confint() on the function in questino 2, we are 95% certain that the slope for 1/MAPE falls between 0.92589823 and 1.069356445. The lower limit of the 95% bootstrap confidence interval is only slightly higher than the interval obtained through the second method. However, the upper limit of the 95% bootstrap confidence interval is very slightly lower than the interval obtained through the second method. Consequently the confidence intervals for the slope of 1/MAPE are extremely similar. 

```{r}
summary(m2)
summary(m1)

simple_mod <- function(MAPE) {
  1/MAPE
}

second_mod <- function(MAPE) {
  -0.007815 +  (0.997627 * 1/MAPE)
}

first_mod <- function(MAPE) {
    0.1383475 + (-0.0045885 * MAPE)
  }

ggplot(d, aes(x = MAPE, y = Return_10_fwd)) + geom_point() + stat_function(fun = simple_mod, color = "red") + stat_function(fun = second_mod, color = "blue") + stat_function(fun = first_mod, color = "green") 
```
All three models capture the mostly negatively correlated relationship between MAPE and the average rate of return over the next 10 yeras of the original dataset. However, Model 2 and the simple model capture a slighltly curvilinear relationship betwee MAPE and the rate of return. Comparatively, Model 1 represents a linear relationship between MAPE and the rate of return. As MAPE increases, the average rate of return decrease consistently. 


#### The big picture
```{r}
mean(betas)
```

1. Since Model 2 has the lowest CV MSE of 0.0018769, I would select the model to make predictions on returns. Looking at the plot from question 5, Model 2 is able to capture the slighty curvilinear relationship between MAPE and the average rate of returns, subsequently seeming to be a pretty good model. Furthermore, in contrast to Model 1, Model 2 better estimates the higher average rate of returns when MAPE (the ration between "Price" and "Earnings") is small. However, Model 2 does not model the slight curvature of the data accurately, subsequently tending to overestimate the average rate of returns when MAPE is larger. 

2.Based off my bootstrapping procedure for the slope of the linear model using 1/MAPE as a predictor, the simple-minded model is very plausible given the data. It follows from the suggestion that the average rate of return over the next 10 years is equal to 1/MAPE that the slope of the simple-minded model is 1. This is very close to the mean of the slope estimates determined from the bootstrap distribution of slope for 1/MAPE in Model 2 (0.9980164). 


#### Chapter 5 exercises

4. The standard deviation of the predicted observation can be estimatted through the bootstrap approach. Correspondingly, we would generate many random samples from our original dataset, sampling for replacement each time, and record the estimates for the standard deviation each time. Afterwards, we would find the mean of the estimates from each time and the standard deviation across all estimates. 


8. 
```{r}
set.seed(1)
x = rnorm(100)
y = x-2 * x^2 + rnorm(100)

plot(x,y)

#c
library(boot)
set.seed(2)
Data <- data.frame(x, y)
fit1 <- glm(y ~ x)
cv.glm(Data, fit1)$delta[1]

fit2 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit2)$delta[1]

fit3 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit3)$delta[1]

fit4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit4)$delta[1]


set.seed(19)
fit5 <- glm(y ~ x)
cv.glm(Data, fit5)$delta[1]

fit6 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit6)$delta[1]

fit7 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit7)$delta[1]

fit8 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit8)$delta[1]

summary(fit1)
summary(fit2)
summary(fit3)
summary(fit4)
```

n = 100 and p = 2
Y = X - 2$X^2$ + $\epsilon$

b) There is a curved relationship between x and y. Initially, there is a positively correlated relationship between x and y as y increases as x increases. However, once x is greater than 0, there is a negatively correlated relationship between x and y as an increase in x is assocaited with a decrease in y. 

c) Compute LOOCV errors:
i. 7.288162
ii. 0.9374236
iii.0.9566218
iv. 0.9539049

d) Compute LOOCV errors:
i. 7.288162
ii. 0.9374236
iii.0.9566218
iv. 0.9539049

The results are exactly the same as the results from (c). This is because LOOCV uses a single observation as a validation set.

e) "Fit2" had the smallest LOOCV error. This is not surprising because the scatterplot from b demonstrates that the relationship between x and y is quadratic. 

f) After fitting each of the models, it is clear that the linear and quadratic terms are consistently statistically significants. However, the cubic and 4th degree terms are not. Consequently, the results agree with our cross-validation results in which the quadratic ("fit2") model had the least LOOCV error. 